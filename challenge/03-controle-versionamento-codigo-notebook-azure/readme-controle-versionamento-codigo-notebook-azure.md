# ğŸš€ AutomaÃ§Ã£o, AnÃ¡lise e Controle de VersÃ£o com Azure Databricks

Este projeto demonstra como **automatizar a criaÃ§Ã£o de ambientes no Azure Databricks**, aplicar **anÃ¡lise de dados com Apache Spark**, e controlar versÃµes com **GitHub** â€” tudo com **Shell Scripts, REST API e prÃ¡ticas modernas** de Data Engineering.


## ğŸ’¡ IntroduÃ§Ã£o ao Azure Databricks

O [**Azure Databricks**](https://learn.microsoft.com/en-us/azure/databricks/) Ã© uma plataforma de anÃ¡lise de dados baseada no **Apache Spark**, totalmente integrada ao Azure. Ela oferece:

- Notebooks colaborativos
- Suporte a linguagens como Python, SQL e Scala
- IntegraÃ§Ã£o com GitHub
- Autoscaling de clusters
- IA assistente integrada para anÃ¡lises


## ğŸ› ï¸ Etapa 1 â€“ Provisionamento Automatizado com Azure CLI

CriaÃ§Ã£o automÃ¡tica de ambiente via **Shell Script**:

```bash
# VariÃ¡veis de ambiente
RESOURCE_GROUP="rg-databricks-demo"
LOCATION="eastus"
DATABRICKS_WS_NAME="dbw-demo-$(date +%s)"

# Criar Resource Group
az group create --name $RESOURCE_GROUP --location $LOCATION

# Criar Workspace do Databricks
az databricks workspace create \
  --resource-group $RESOURCE_GROUP \
  --name $DATABRICKS_WS_NAME \
  --location $LOCATION \
  --sku standard

echo "Databricks Workspace '$DATABRICKS_WS_NAME' criado com sucesso."
```

<figure style="text-align: center;">
    <img src="./figure/figure01-status-databricks.png" alt="Verificando o status do workspace do Databricks criado via shell." width="3500" height="200">
    <figcaption>Verificando o status do workspace do Databricks criado via shell.</figcaption>
</figure>

<figure style="text-align: center;">
    <img src="./figure/figure02-status-databricks-panel.png" alt="Workspace do Databricks criado via shell." width="3500" height="500">
    <figcaption>Workspace do Databricks criado via shell.</figcaption>
</figure>


## ğŸ”‘ Etapa 2 â€“ Gerando Token de Acesso Seguro

Para interagir com a API do Databricks, gere seu token de acesso:

1. Acesse: `https://<workspace>.azuredatabricks.net`
2. Perfil â†’ **User Settings** â†’ **Access Tokens**
3. Clique em **Generate New Token**
4. Nomeie e defina expiraÃ§Ã£o
5. Copie e armazene com seguranÃ§a

### ğŸ’¡ Dica Profissional

Evite hardcode de tokens em scripts. Use variÃ¡veis de ambiente:

```bash
export DATABRICKS_TOKEN="seu-token-aqui"
```


## âš™ï¸ Etapa 3 â€“ CriaÃ§Ã£o de Cluster via API REST

CriaÃ§Ã£o de cluster usando `curl` com autenticaÃ§Ã£o:

```bash
DATABRICKS_HOST="https://$DATABRICKS_WS_NAME.azuredatabricks.net"
DATABRICKS_TOKEN="<SEU_TOKEN_DATABRICKS>"

curl -X POST $DATABRICKS_HOST/api/2.0/clusters/create \
  -H "Authorization: Bearer $DATABRICKS_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
        "cluster_name": "demo-cluster",
        "spark_version": "12.2.x-scala2.12",
        "node_type_id": "Standard_DS3_v2",
        "num_workers": 2,
        "autotermination_minutes": 20
      }'
```

Consulta de cluster criado:

```bash
curl -s -X GET "$DATABRICKS_HOST/api/2.0/clusters/list" \
  -H "Authorization: Bearer $DATABRICKS_TOKEN" \
  | jq '.clusters[] | {cluster_name, cluster_id, state}'
```

<figure style="text-align: center;">
    <img src="./figure/figure03-dtatus-cluster-demo-cluster.png" alt="Consulta do demo-cluster criado." width="3500" height="200">
    <figcaption>Consulta do demo-cluster criado.</figcaption>
</figure>


## ğŸ“Š Etapa 4 â€“ AnÃ¡lise de Dados com Apache Spark + IA

Notebook de anÃ¡lise com **Spark**:

```python
# Nome do notebook: Analise_Dados_Spark.py

df = spark.read.csv("/databricks-datasets/samples/population-vs-price/data_geo.csv", header=True, inferSchema=True)

df.show(5)
df.describe().show()

from pyspark.sql.functions import col
df.select(col("2014 Population"), col("2015 median sales price")).show()
```

<figure style="text-align: center;">
    <img src="./figure/figure04-analise-codigo-teste-IA.png" alt="Utilizando o assistente de IA." width="3500" height="500">
    <figcaption>Utilizando o assistente de IA.</figcaption>
</figure>

### âœ… Melhorias com IA do Databricks:

| Antes | Depois (com IA) | BenefÃ­cio |
|-------|------------------|-----------|
| `df.show(5)` | `display(df.limit(5))` | Visual interativo com filtros |
| `df.describe().show()` | `display(df.describe())` | EstatÃ­sticas mais legÃ­veis |
| `df.select(...)` | `display(...)` | GeraÃ§Ã£o automÃ¡tica de grÃ¡ficos |


## ğŸ” Etapa 5 â€“ Controle de VersÃ£o com GitHub

IntegraÃ§Ã£o com GitHub direto do notebook:

1. No notebook â†’ **Git** â†’ **Link notebook to Git repo**
2. Configure com seu GitHub Token
3. FaÃ§a commits e versionamento pelo Databricks


## âœ… Resumo Profissional

Este projeto demonstra na prÃ¡tica:

- CriaÃ§Ã£o de ambientes com **Azure CLI**
- Uso da **API REST do Databricks**
- AnÃ¡lise de dados com **Apache Spark**
- VisualizaÃ§Ãµes e insights com **IA integrada**
- IntegraÃ§Ã£o com **GitHub** para versionamento


## ğŸ’¡ Insights Relevantes

- âš¡ **AutomaÃ§Ã£o acelera o setup** e reduz erros em ambientes de dados
- ğŸ” **display() melhora a experiÃªncia visual** sem dependÃªncia de ferramentas externas
- ğŸ¤– **IA do Databricks ajuda a aplicar boas prÃ¡ticas**
- ğŸ” **GitHub permite colaboraÃ§Ã£o e rastreabilidade do cÃ³digo**


## ğŸš€ PrÃ³ximos Passos (SugestÃµes Profissionais)

- Integrar com **Azure Data Lake** para dados em produÃ§Ã£o  
- Criar dashboards em **Power BI** conectados ao Databricks  
- Testar o uso de **Delta Lake** e controle de versÃ£o dos dados  
- Explorar **MLflow** para experimentos de Machine Learning  


## ğŸ“š ReferÃªncias TÃ©cnicas

- ğŸ“˜ [Microsoft Learn: Transform Data in Azure Databricks](https://microsoftlearning.github.io/mslearn-databricks/Instructions/Exercises/LA-03-Transform-data.html)  
- ğŸ“˜ [DocumentaÃ§Ã£o Oficial do Azure Databricks](https://learn.microsoft.com/en-us/azure/databricks/)  
- ğŸ§  [API REST Databricks](https://docs.databricks.com/api/index.html)  
- ğŸ§ª [Dataset population-vs-price](https://www.databricks.com/resources/datasets/population-vs-price)  
- ğŸš [Azure CLI Docs](https://learn.microsoft.com/pt-br/cli/azure/install-azure-cli)  
