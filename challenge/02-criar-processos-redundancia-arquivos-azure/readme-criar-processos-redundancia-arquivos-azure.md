# Criando Processos de RedundÃ¢ncia de Arquivos na Azure

## ğŸ“ Parte 1: SoluÃ§Ã£o Automatizada do Desafio

### ğŸ’¾ RedundÃ¢ncia de Arquivos na Azure com Data Factory e Databricks

### ğŸ¯ Objetivo
Implementar uma soluÃ§Ã£o de redundÃ¢ncia de arquivos utilizando os serviÃ§os da Azure, copiando arquivos de uma fonte on-premise para um Data Lake, utilizando Data Factory, integraÃ§Ã£o com SQL Database e organizaÃ§Ã£o em camadas (bronze, prata, ouro).


### ğŸ§± Recursos Criados

1. **Azure Data Factory**
2. **SQL Database**
3. **SQL Server**
4. **Storage Account (Data Lake Gen2 habilitado)**


### ğŸ› ï¸ Etapas e Scripts

#### 1. ğŸ”§ CriaÃ§Ã£o dos Recursos via CLI

```bash
# VariÃ¡veis
RG="rg-redundancia"
LOCATION="eastus"
STORAGE_NAME="redundanciastorage$RANDOM"
DATAFACTORY_NAME="redundanciaADF$RANDOM"
SQL_SERVER_NAME="redundanciasqlsrv$RANDOM"
SQL_DB_NAME="redundanciadb"

# Criar grupo de recursos
az group create --name $RG --location $LOCATION

# Criar Storage Account com Data Lake Gen2
az storage account create \
  --name $STORAGE_NAME \
  --resource-group $RG \
  --location $LOCATION \
  --sku Standard_LRS \
  --kind StorageV2 \
  --hierarchical-namespace true

# Criar Azure SQL Server
az sql server create \
  --name $SQL_SERVER_NAME \
  --resource-group $RG \
  --location $LOCATION \
  --admin-user sqladmin \
  --admin-password "YourP@ssword123"

# Criar SQL Database
az sql db create \
  --resource-group $RG \
  --server $SQL_SERVER_NAME \
  --name $SQL_DB_NAME \
  --service-objective S0

# Criar Azure Data Factory
az datafactory create \
  --resource-group $RG \
  --factory-name $DATAFACTORY_NAME \
  --location $LOCATION
```


#### 2. ğŸ“ OrganizaÃ§Ã£o do Data Lake

```bash
# Criar containers e diretÃ³rios bronze, prata e ouro
az storage container create --name raw --account-name $STORAGE_NAME
az storage fs directory create -n bronze --file-system raw --account-name $STORAGE_NAME
az storage fs directory create -n prata --file-system raw --account-name $STORAGE_NAME
az storage fs directory create -n ouro --file-system raw --account-name $STORAGE_NAME
```


#### 3. ğŸ”Œ ConfiguraÃ§Ã£o no Data Factory

- Criar **Linked Services**:
  - Azure SQL Database (On-Prem via Integration Runtime)
  - Azure Data Lake Storage Gen2

- Criar um **Integration Runtime Self-hosted** para acesso on-premise:
  - Instalar e configurar localmente (https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime)


#### 4. ğŸ§ª Pipeline de CÃ³pia

- Criar pipeline no Data Factory:
  - Fonte: Tabela `produtos` no SQL Database on-premise
  - Destino: Data Lake â€“ diretÃ³rio `bronze/produtos.txt`
  - Usar atividade **Copy Data**
  - Configurar transformaÃ§Ã£o opcional (ex: renomear colunas ou filtrar)


### ğŸ§  Boas PrÃ¡ticas

- SeparaÃ§Ã£o por camadas (bronze, prata, ouro) permite versionamento e governanÃ§a de dados.
- Use triggers no Data Factory para automatizar execuÃ§Ãµes.
- Versione seu pipeline com JSON exportado via Azure DevOps.


### ğŸ“š ReferÃªncias Importantes

- [DocumentaÃ§Ã£o Azure Data Factory](https://learn.microsoft.com/azure/data-factory/introduction)
- [Integration Runtime Self-hosted](https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime)
- [Camadas de dados: bronze, prata, ouro](https://learn.microsoft.com/en-us/azure/architecture/example-scenario/data/data-lake)
- [CLI de Azure](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli)


### ğŸš€ ConclusÃ£o

Este projeto configura uma pipeline completa de redundÃ¢ncia e ingestÃ£o de dados. Os dados saem de uma base on-premise e sÃ£o organizados em um Data Lake moderno, prontos para processamento com Databricks, Synapse ou Power BI.


## ğŸ“ Parte 2: Artigo â€“ Insights do Desafio: RedundÃ¢ncia de Arquivos na Azure


### Desafio Microsoft AI for Tech: RedundÃ¢ncia de Arquivos na Azure

#### ğŸ“Œ Contexto

Durante o bootcamp **Microsoft AI for Tech**, realizamos um desafio com foco em engenharia de dados utilizando a Azure. A missÃ£o? Criar um fluxo de redundÃ¢ncia de dados a partir de uma base on-premise, organizando tudo em um Data Lake e utilizando o Azure Data Factory como motor de orquestraÃ§Ã£o.


#### ğŸ› ï¸ A Arquitetura

Os serviÃ§os principais que compÃµem a arquitetura sÃ£o:

- **Azure Data Factory**: responsÃ¡vel pela orquestraÃ§Ã£o dos dados.
- **SQL Server (on-premise)**: a fonte original dos dados.
- **Storage Account com Data Lake Gen2**: destino dos arquivos.
- **Integration Runtime Self-hosted**: ponte segura entre o ambiente local e a nuvem.


#### ğŸ”„ Processo de CÃ³pia

1. **CriaÃ§Ã£o dos Recursos** â€“ Utilizamos a CLI da Azure para criar todos os recursos, o que traz agilidade e padronizaÃ§Ã£o.
2. **ConexÃ£o com Fonte On-Premise** â€“ Foi necessÃ¡rio instalar e configurar um Integration Runtime local. Essa parte foi um pouco mais trabalhosa, mas essencial para o processo.
3. **Pipeline de CÃ³pia** â€“ Criamos um pipeline no Data Factory que extrai os dados da tabela `produtos`, transforma para `.txt` e armazena na camada bronze do Data Lake.


#### ğŸ’¡ Insights e Aprendizados

- **ModularizaÃ§Ã£o Ã© tudo**: trabalhar com camadas (bronze, prata e ouro) permite maior controle da qualidade dos dados.
- **A automaÃ§Ã£o vale a pena**: scripts com Azure CLI economizam tempo e reduzem erros.
- **SeguranÃ§a importa**: o uso do self-hosted IR traz robustez Ã  comunicaÃ§Ã£o com bases locais.
- **Escalabilidade**: essa estrutura estÃ¡ pronta para ser escalada para milhÃµes de registros e mÃºltiplas tabelas.


#### ğŸ”® PrÃ³ximos Passos

- Integrar com Databricks para transformar os dados da camada bronze para prata e ouro.
- Criar visualizaÃ§Ãµes com Power BI conectando diretamente ao Lakehouse.
- Automatizar as execuÃ§Ãµes com triggers baseadas em tempo ou eventos.


#### ğŸ“˜ ConclusÃ£o

Esse desafio mostrou como Ã© possÃ­vel unir diferentes serviÃ§os da Azure para criar uma arquitetura robusta e escalÃ¡vel de ingestÃ£o de dados. Com poucos cliques (ou comandos), vocÃª sai de uma base local para uma estrutura moderna de Data Lake, pronta para anÃ¡lise e inteligÃªncia artificial.

